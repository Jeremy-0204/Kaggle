{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import tifffile as tiff\n",
    "import cv2\n",
    "import torch.nn as nn\n",
    "import albumentations as A\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Original Image & Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/kaggle/input/blood-vessel-segmentation/train'\n",
    "dataset = 'kidney_1_dense'\n",
    "\n",
    "images_path = os.path.join(base_path, dataset, 'images')\n",
    "labels_path = os.path.join(base_path, dataset, 'labels')\n",
    "\n",
    "image_files = sorted([os.path.join(images_path, f) for f in os.listdir(images_path) if f.endswith('.tif')])\n",
    "label_files = sorted([os.path.join(labels_path, f) for f in os.listdir(labels_path) if f.endswith('.tif')])\n",
    "\n",
    "def show_images(images, titles = None, cmap = 'gray'):\n",
    "    n = len(images)\n",
    "    fig, axes = plt.subplots(1, n, figsize = (20, 10))\n",
    "    if not isinstance(axes, np.ndarray):\n",
    "        axes = [axes]\n",
    "    for idx, ax in enumerate(axes):\n",
    "        ax.imshow(images[idx], cmap = cmap)\n",
    "        if titles:\n",
    "            ax.set_title(titles[idx])\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "first_image = tiff.imread(image_files[981])\n",
    "first_label = tiff.imread(label_files[981])\n",
    "\n",
    "show_images([first_image, first_label], titles = ['First Image', 'First Label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_files, mask_files, input_size = (256, 256), augmentation_transforms = None):\n",
    "        self.image_files = image_files\n",
    "        self.mask_files = mask_files\n",
    "        self.input_size = input_size\n",
    "        self.augmentation_transforms = augmentation_transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_files[idx]\n",
    "        mask_path = self.mask_files[idx]\n",
    "        \n",
    "        image = preprocess_image(image_path)\n",
    "        mask = preprocess_mask(mask_path)\n",
    "        \n",
    "        if self.augmentation_transforms:\n",
    "            image, mask = self.augmentation_transformsaug(image, mask)\n",
    "            \n",
    "        return image, mask\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing of Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(path):\n",
    "    img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
    "    img = np.tile(img[...,None], [1, 1, 3])\n",
    "    img = img.astype('float32')\n",
    "    mx = np.max(img)\n",
    "    if mx:\n",
    "        img /= mx\n",
    "    \n",
    "    img = np.transpose(img, (2, 0, 1))\n",
    "    img_ten = torch.tensor(img)\n",
    "    return img_ten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing of Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_mask(path):\n",
    "    msk = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
    "    msk = np.tile(img[..., None], [1, 1, 3])\n",
    "    msk /= 255.0\n",
    "    msk_ten = torch.tensor(msk)\n",
    "    return msk_ten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_image(image, mask):\n",
    "    image_np = image.permute(1, 2, 0).numpy()\n",
    "    mask_np = mask.numpy()\n",
    "    \n",
    "    transform = A.Compose([\n",
    "        A.Resize(256, 256, interpolation = cv2.INTER_NEAREST),\n",
    "        A.HorizontalFlip(p = 0.5),\n",
    "        A.VerticalFlip(p = 0.5),\n",
    "        A.ShiftScaleRotate(scale_limit = 0.5, rotate_limit = 0, shift_limit = 0.1, p = 1, border_mode = 0),\n",
    "        A.RandomCrop(height = 256, width = 256, always_apply = True),\n",
    "        A.RandomBrightness(p = 1),\n",
    "        A.OneOf(\n",
    "            [\n",
    "                A.Blur(blur_limit = 3, p = 1),\n",
    "                A.MotionBlur(blur_limit = 3, p = 1),\n",
    "            ],\n",
    "            p = 0.9,\n",
    "        ),\n",
    "    ])\n",
    "    \n",
    "    augmented = transform(image = image_np, mask = mask_np)\n",
    "    augmented_image, augmented_mask = augmented['image'], augmented['mask']\n",
    "    \n",
    "    augmented_image = torch.tensor(augmented_image, dtype = torch.float32).permute(2, 0, 1)\n",
    "    augmented_mask = torch.tensor(augmented_mask, dtype = torch.float32)\n",
    "    \n",
    "    return augmented_image, augmented_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_files, val_image_files, train_mask_files, val_mask_files = train_test_split(image_files, label_files, test_size = 0.2, random_state = 42)\n",
    "\n",
    "train_dataset = CustomDataset(train_image_files, train_mask_files, augmentation_transforms = augment_image)\n",
    "val_dataset = CustomDataset(val_image_files, val_mask_files, augmentation_transforms = augment_image)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = 8, shuffle = True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size = 8, shuffle = False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
